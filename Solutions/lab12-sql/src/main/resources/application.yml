spring:
  application.name: Lab11 SQL Generation
  main.web-application-type: none     # Do not start a web server.

  ai:
    retry:
      max-attempts: 1           # Maximum number of retry attempts.
      on-client-errors: false   # If false, throw a NonTransientAiException, and do not attempt retry for 4xx client error codes.

    bedrock.converse.chat.enabled:  false # Disable all models by default.
    ollama.chat.enabled:            false # Disable all models by default.
    openai.chat.enabled:            false # Disable all models by default.
    azure.openai.chat.enabled:      false # Disable all models by default.

# TODO-02: Adjust the settings below based on the chat model you plan to use.
# Use previous labs for guidance.
#
# If you plan to use Amazon Bedrock: 
#   - Adjust the region setting if needed.
#   - Set the spring.ai.bedrock.converse.chat.enabled to true to enable the chat model.
#   - Makes sure you have enabled the Anthropic Claude 3.5 Sonnet model
#   - Set spring.ai.bedrock.converse.chat.options.model to "anthropic.claude-3-5-sonnet-20240620-v1:0" or equivalent (make sure it is enabled)
#   - Adjust the model if desired or take the default.  See https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html for latest list.
#
# If you plan to use OpenAI:
#   - Set spring.ai.openai.chat.enabled to true to enable the chat model.
#   - Set spring.ai.openai.chat.options.model to "gpt-35-turbo", or see https://platform.openai.com/docs/models/embeddings for latest list.
#
# If you plan to use Azure OpenAI:
#   - Set spring.ai.azure.openai.chat.enabled to true to enable the chat model.
#   - Set spring.ai.azure.openai.endpoint to the value you established during Azure setup.
#   - Set spring.ai.azure.openai.chat.options.deployment-name to the value you establised during setup.
#   - Set spring.ai.azure.openai.chat.options.model to "gpt-35-turbo", or whichever model you have enabled.
#
# If you plan to use Ollama:
#   - Set spring.ai.ollama.chat.enabled to true to enable the chat model.
#   - Adjust the base-url if needed.
#   - Make sure Ollama is running.
#
# Note that only one model will be active at a time.  The active model is determined by the active profile.



---
spring:
  config.activate.on-profile: aws

  ai:
    bedrock:
      aws.region: us-west-2 # Adjust as needed.
      converse:
        chat:
          enabled: true
          options:
            model: anthropic.claude-3-5-sonnet-20240620-v1:0 # Adjust as needed.

---
spring:
  config.activate.on-profile: openai

  ai:
    openai:
      chat:
        enabled: true
        options:
          model: gpt-3.5-turbo


---
spring:
  config.activate.on-profile: azure

  ai:
    azure:
      openai:
        endpoint: ENDPOINT-GOES-HERE
        chat:
          enabled: true
          options:
            deployment-name: DEPLOYMENT-NAME-GOES-HERE
            model: gpt-35-turbo        

---
spring:
  config.activate.on-profile: ollama

  ai:
    ollama:
      base-url: http://localhost:11434  # Default base URL when you run Ollama from Docker
      chat:
        enabled: true
