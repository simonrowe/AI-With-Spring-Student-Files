spring:
  application.name: Lab10 RAG
  main.web-application-type: none     # Do not start a web server.

  ai:
    retry:
      max-attempts: 1      # Maximum number of retry attempts.
      on-client-errors: false   # If false, throw a NonTransientAiException, and do not attempt retry for 4xx client error codes.

    openai.embedding.enabled:   false # Allow profile to enable this if needed.
    ollama.embedding.enabled:   false # Allow profile to enable this if needed.
    bedrock.titan.chat.enabled: false # Allow profile to enable this if needed.
    openai.chat.enabled:        false # Allow profile to enable this if needed.
    azure.openai.chat.enabled:  false # Allow profile to enable this if needed.
    ollama.chat.enabled:        false # Allow profile to enable this if needed.

# TODO-03: Adjust the settings below based on the chat model you plan to use.
# Use previous labs for guidance.
#
# If you plan to use Amazon Bedrock: 
#   - Adjust the region setting if needed.
#   - Set the spring.ai.bedrock.titan.chat.enabled to true to specify use of the Titan chat model.
#   - Use the default model or experiment with alternatives.  See https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html for latest list.
#
# If you plan to use OpenAI:
#   - Set spring.ai.openai.chat.enabled to true to enable the chat model.
#   - Use the default model or experiment with alternatives.  See https://platform.openai.com/docs/models/embeddings for latest list.
#
# If you plan to use Azure OpenAI:
#   - Set spring.ai.azure.openai.chat.enabled to true to enable the chat model.
#   - Set spring.ai.azure.openai.endpoint to the value you established during Azure setup.
#   - Set spring.ai.azure.openai.chat.options.deployment-name to the value you establised during setup.
#   - Set spring.ai.azure.openai.chat.options.model to "gpt-35-turbo", or whichever model you have enabled.
#
# If you plan to use Ollama:
#   - Set spring.ai.ollama.chat.enabled to true to enable the chat model.
#   - Adjust the base-url if needed.
#   - Make sure Ollama is running.
#   - Use the default model or experiment with alternatives.  See https://ollama.com/library?sort=popular
#   - Make sure you have pulled the model you wish to use.
#
# Note that only one model will be active at a time.  The active model is determined by the active profile.


---
spring:
  config.activate.on-profile: aws

  ai:
    bedrock:
      titan:
        chat:
          enabled: true

---
spring:
  config.activate.on-profile: openai

  ai:
    openai:
      chat:
        enabled: true

---
spring:
  config.activate.on-profile: azure

  ai:
    azure:
      openai:
        chat:
          enabled: true

---
spring:
  config.activate.on-profile: ollama

  ai:
    ollama:
      chat:
        enabled: true




---
spring:
  config.activate.on-profile: simple-vector-store
  
  # If running a simple vector store, exclude the Redis, PGVector, and DataSource autoconfigurations.
  autoconfigure.exclude: 
  - org.springframework.ai.autoconfigure.vectorstore.redis.RedisVectorStoreAutoConfiguration
  - org.springframework.ai.autoconfigure.vectorstore.pgvector.PgVectorStoreAutoConfiguration
  - org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration

---
spring:
  config.activate.on-profile: redis-vector-store

  # TODO-22 (OPTIONAL): Set properties for the Redis vector store.
  # Note that these will only be used if the "redis-vector-store" profile is active.
  # Set spring.ai.vectorstore.redis.uri to redis://localhost:6379, unless your redis is running elsewhere.
  # Set spring.ai.vectorstore.redis.initializeSchema to true to create the necessary schema.
  # (You may not want to do this in a production environment.)
  ai:
    vectorstore:
      redis:
        index: default-index         # Default.
        prefix: "default:"           # Default.

  # If running a redis vector store, exclude the PGVector and DataSource autoconfigurations.
  autoconfigure.exclude: 
  - org.springframework.ai.autoconfigure.vectorstore.pgvector.PgVectorStoreAutoConfiguration
  - org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration

---
spring:
  config.activate.on-profile: pg-vector-store

  # TODO-25 (OPTIONAL): Set properties for the Postgres vector store.
  # Note that these will only be used if the "pg-vector-store" profile is active.
  # Set spring.datasource.url to jdbc:postgresql://localhost:5432/postgres, unless your Postgres is running elsewhere.
  # Set spring.datasource.username and spring.datasource.password the username and password set when running the container.
  # Set the spring.ai.vectorstore.pgvector.initialize-schema to true to create the necessary schema.
  # (You may not want to do this in a production environment.)
  # Set the spring.ai.vectorstore.pgvector.index-type to HNSW.
  # Set the spring.ai.vectorstore.pgvector.distance-type to COSINE_DISTANCE.
  # Set the spring.ai.vectorstore.pgvector.dimensions based on the the underlying model you are using
  #    For SpringAI's internal implementation, use 384
  #    For AWS/Bedrock/Cohere, use 1024
  #    For OpenAI's text-embedding-ada-002, use 1536
  #    If you make a mistake here, don't worry.  The error message will tell you the correct #.  Delete and restart the DB with the revised number.


  datasource:
  ai:
    vectorstore:
      pgvector:
        # CRITICAL: The PGVectorStore database must be initialized to the # of dimensions in the model you are using.
        dimensions: 1536                # OpenAI's text-embedding-ada-002 model uses 1536 dimensions.
        #dimensions: 1024                 # AWS/Bedrock/Cohere uses 1024 dimensions.

  # If running a simple vector store, exclude the Redis auto configuration.
  autoconfigure.exclude: 
  - org.springframework.ai.autoconfigure.vectorstore.redis.RedisVectorStoreAutoConfiguration

