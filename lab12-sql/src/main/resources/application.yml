spring:
  application.name: Lab11 SQL Generation
  main.web-application-type: none     # Do not start a web server.

  ai:
    retry:
      max-attempts: 1           # Maximum number of retry attempts.
      on-client-errors: false   # If false, throw a NonTransientAiException, and do not attempt retry for 4xx client error codes.

    bedrock.anthropic3.chat.enabled: false # Disable all models by default.
    ollama.chat.enabled:            false # Disable all models by default.
    openai.chat.enabled:            false # Disable all models by default.
    azure.openai.chat.enabled:      false # Disable all models by default.

# TODO-02: Adjust the settings below based on the chat model you plan to use.
# Use previous labs for guidance.
#
# If you plan to use Amazon Bedrock: 
#   - Adjust the region setting if needed.
#   - Set the spring.ai.bedrock.anthropic3.chat.enabled to true to enable the chat model.
#   - Makes sure you have enabled the Anthropic Claude 3 Sonnet model
#   - Adjust the model if desired or take the default.  See https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html for latest list.
#
# If you plan to use OpenAI:
#   - Set spring.ai.openai.chat.enabled to true to enable the chat model.
#   - Set spring.ai.openai.chat.options.model to "gpt-35-turbo", or see https://platform.openai.com/docs/models/embeddings for latest list.
#
# If you plan to use Azure OpenAI:
#   - Set spring.ai.azure.openai.chat.enabled to true to enable the chat model.
#   - Set spring.ai.azure.openai.endpoint to the value you established during Azure setup.
#   - Set spring.ai.azure.openai.chat.options.deployment-name to the value you establised during setup.
#   - Set spring.ai.azure.openai.chat.options.model to "gpt-35-turbo", or whichever model you have enabled.
#
# If you plan to use Ollama:
#   - Set spring.ai.ollama.chat.enabled to true to enable the chat model.
#   - Adjust the base-url if needed.
#   - Make sure Ollama is running.
#
# Note that only one model will be active at a time.  The active model is determined by the active profile.



---
spring:
  config.activate.on-profile: aws

  ai:
    bedrock:
      anthropic3:
        chat:
          enabled: true

---
spring:
  config.activate.on-profile: openai

  ai:
    openai:
      chat:
        enabled: true


---
spring:
  config.activate.on-profile: azure

  ai:
    azure:
      openai:
        endpoint: ENDPOINT-GOES-HERE
        chat:
          enabled: true
          options:
            deployment-name: DEPLOYMENT-NAME-GOES-HERE


---
spring:
  config.activate.on-profile: ollama

  ai:
    ollama:
      chat:
        enabled: true
